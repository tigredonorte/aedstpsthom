%	Documentação do Trabalho Prático 0 de AEDSIII
%	@Thompson Moreira Filgueiras
%
%	* Você pode identificar erros de grafia através do seguinte comando linux:
%		aspell --encoding="iso8859-1" -c -t=tex --lang="pt_BR" tp0.tex
%	
%	Tenha cuidado com problemas de codificação, você pode perder muito tempo com isso (ter que reescrever o texto por que os caracteres % acendutados não aparecem corretamento no pdf, por exemplo). Se você usa Vi/Vim, ele identifica a codificação sozinho, em editores do tipo % Kate/Kwrite você pode definir a codificação em Ferramentas/Codificação, escolha a opção Oeste Europeu (iso8859-1).
%	Para compilar o texto utilize o comando make (foi criado um Makefile)
%	Para maiores informações consulte referências sobre Latex

\documentclass[12pt]{article}
\usepackage{sbc-template}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{times,amsmath,epsfig}
\usepackage{graphicx,url}
 \makeatletter
 \newif\if@restonecol
 \makeatother
 \let\algorithm\relax
 \let\endalgorithm\relax 
\usepackage[lined,algonl,ruled]{algorithm2e}
\usepackage{multirow}
\usepackage[brazil]{babel}   
\usepackage[latin1]{inputenc}  

\sloppy

\title{TRABALHO PRÁTICO 1: \\ Máquina de Busca}

\author{Thompson Moreira Filgueiras}

\address{Departamento de Ciência da Computação -- Universidade Federal de Minas Gerais (UFMG)
\email{thom@dcc.ufmg.br}
}

\begin{document} 

\maketitle

\begin{resumo} 
Este relatório descreve o funcionamento de um índice invertido, assim como sua estrutura interna e faz alguns comentários sobre uma máquina de busca mais complexa. Uma vez que este trabalho prático consistiu em primeiro lugar a implementar uma máquina simples. Outro ponto abordado durante o relatório é a utilização de pthreads, este é o principal objetivo do programa. Ao final o programa avalia o tempo medio de latencia de cada thread, que é um parâmetro bastante comum quando são desenvolvidas máquinas de busca.
\end{resumo}

\section{INTRODUÇÃO}

Neste trabalho prático foi implementada uma máquina de busca simples, que consiste em inserir em uma estrutura de dados todas as palavras de vários documentos. No caso específico deste trabalho os documentos inseridos eram randômicos, criados por um processo à parte(thread) durante a execussão do código. Os principais objetivos do trabalho são: implementar uma máquina de busca e utilizar as chamadas pthreads.

	O problema específico de implementar uma máquina de busca, consiste em criar uma estrutura de dados na qual possa ser inseridas várias palavras, esta estrutura pode variar dependendo da aplicação: pode ser uma fila, uma arvore binária (ou outra arvore qualquer), um hash e suas variações, dentre outros. Esta estrutura deve suportar inserções e pesquisas de termos, sendo que estes podem ocorrer simultaneamente no mudo real. Neste trabalho a tarefa de inserir e pesquisar simultaneamente foi realizada pela criação de várias threads, o que tornou o programa um pouco mais proximo desta realidade. Uma explicação mais detalhada sobre máquina de busca pode ser encontrada: \cite{Motor de Busca} 

	A criação de um índice invertido deve permitir a busca por frases que mostre dentro do conjunto de documentos, qual o documento que possui aquelas palavras. Estes índices fazem com que a busca de termos seja mais eficiente, com o overhead de necessitar de uma memoria adicional que pode chegar a atingir o tamanho dos documentos. No programa implementado, esta memória é igual a soma de todos os documentos. O indice invertido, necessita ainda que sua resposta seja a mais atualizada possível, pois, os documentos podem ser criados, alterados ou simplesmente excluídos e para que uma busca seja eficaz o retorno desta pesquisa tem que levar em conta essas modificações em tempo real. Uma explicação mais detalhada sobre índice invertido pode ser encontrada: \cite{Indice} 

	O outro problema abordado consiste na implementação dessas threads, assim como seu controle, para evitar que vários problemas possíveis ocorram, tais como: duas threads criarem ponteiros simultaneamente para uma nova posição e uma destas posições se perder; uma thread adiquirir o fluxo de execussão e não liberar para que as outras possam acessar as areas críticas (deathlock), enfim como gerenciar as threads. Uma thread pode ser considerada como um processo a parte em um programa, ou ainda um fluxo de execussão concorrente, com a diferença que as threads compartilham memoria entre si, diferentemente dos processos (a não ser em casos exepcionais em que utilizando algum mecanismo de leitura de memoria isso seja feito). Para mais informações sobre pthreads consulte este tuturial: \cite{PThreads}

	O restante deste relatório é organizado da seguinte forma. A Seção~\ref{trabalhos_relacionados} discute alguns temas relacionados ao trabalho. A Seção~\ref{solucao_proposta} descreve a máquina de busca e a solução proposta. A Seção~\ref{implementacao} trata de detalhes específicos da implementação do trabalho. A Seção~\ref{avaliacao_experimental} contém a avaliação experimental do tempo de latencia das pesquisas. A Seção~\ref{conclusao} conclui o trabalho.

\section{REFERÊNCIAS RELACIONADAS}
\label{trabalhos_relacionados}

Podemos dividir as referências associadas ao problema estudado e à solução proposta dentre os seguintes grupos:
\begin{itemize}
\item \textbf{Técnicas de Compressão Aplicadas à Recuperação de Informação} Estas técnicas permitem que a busca seja feita diretamente sobre o texto comprimido. A principal vantagem disto deve-se ao fato de reduzir o espaço de armazenamento em disco dos termos presentes por exemplo em um índice invertido. Desta forma, o overhead citado na introdução, não será solucionado, mas pelo menos será minimizado com pouca perda ou até nenhuma na eficiência do algoritmo de busca e indexação. Algoritmos para este tipo de busca podem chegar a ser até 8 vezes mais rápidos que o melhor algoritmo de busca encontrado na literatura para busca em textos não comprimidos. Um exemplo de método de compressão é chamado BH (Byte-Huffman) desenvolvido para sistemas de recuperação de informações ou ainda compressões da família Ziv-Lampel. O google por exemplo utiliza algoritmos para reduzir o espaço armazenado em seus índices invertidos, embora não digam qual a técnica utilizada (segredo profissional).
\end{itemize}

\section{SOLUÇÃO PROPOSTA}
\label{solucao_proposta}

A solução proposta para implementar o índice invertido utilizou uma estrura de dados hash e duas filas, a saber: a primeira guarda o cada termo dentro de uma posição do hash, ou seja, a solução para conflitos adotada for o emprego desta primeira fila. A segunda fila usada guarda os identificadores dos documentos que as possuem. A função H, que calcula o indice do termo foi uma adaptação de duas outras funções. Uma delas leva em conta que quanto mais proximo a letra está do inicio da string, maior é a relevância dada à esta string, de tal forma que a soma das outras relevâncias não possa alcançar a relevancia desta. Em outras palavras a função h da um peso a cada elemento igual a 2^(tamanho da string - id em relacao a primeira letra). O segundo método transforma cada chave em um identificador numérico e multiplica cada letra por um peso, neste caso foi multiplicado pelo peso da primeira funcao e a posicao do hash na qual o termo pertencerá sera essa multiplicação (mod n) onde n é o tamanho do hash utilizado. Este tamanho do hash foi arbitrado como um numero primo cerca de 10 vezes maior do que o tamanho máximo da entrada, que no programa é conhecido (aproximadamente 109 mil palavras). Mas mesmo que fosse estourado este tamanho, a estrutura suportaria tranquilamente, no entanto o número de colisões aumentaria e o tempo de latência de uma pesquisa seria aumentado.

Ao solucionar o hash com uma fila um problema foi criado: para um grande número de colisões este hash pode se tornar uma estrutura extremamente lenta. No entanto implementar uma fila é bem simplório e como o tamanho da entrada era conhecido este número grande de colisões não ocorre. 

Ao criar uma fila contendo os ids dos documentos ocorre um certo atraso para recuperar a pesquisa pois esta é sequencial e um outro atraso é criado ao unir duas ou mais filas de uma mesma pesquisa. A inserção na fila de ids de documentos é feita de tal forma que os elementos inseridos estejam em ordem, desta forma ao realizar uma busca de duas palavras, basta inserir todos os elementos das duas filas em uma terceira fila de ids e verificar quantos ids foram inseridos mais de uma vez. Para todo id inserido mais de uma vez, temos um sucesso na pesquisa, pois este elemento foi encontrado na primeira e na segunda fila.

As tarefas geradas pelo gen_data são guardadas em uma fila pela thread produtora e inseridas ou pesquisadas pelas threads consumidoras. A inserção e a remoção de elementos desta fila foi trancada para evitar os problemas gerados pelo uso de threads. O código ainda possui um lock para cada inserção na fila de documentos e para cada inserção na fila de ids dos documentos, pelos mesmos motivos citados.

A seguir, serão descritas as estruturas de dados e algoritmos propostos, assim como a análise de complexidade dos algoritmos:

\subsection{Estruturas de dados}

\subsubsection{fila de tarefas:} 

Armazena todas as tarefas geradas pelo gen_doc. A thread produtora a coloca nesta fila, as threads consumidoras retiram desta fila as tarefas e insere no indice invertido. Esta estrutura so possui as operacoes de inserção e remoção, não realiza a pesquisa. Logo para todas as tarefas desta estrutura ela é $O(1)$. Cada fila possui uma celula que por sua vez possui um item que finalmente possui um terms_t, que é o trabalho gerado pela gen_doc
\begin{algorithm}[h!]
\begin{footnotesize}

	fila
	{
		celula
		{
			termsT
		}
	}
	
\end{footnotesize}
\end{algorithm}

\subsubsection{Dicionario:} 

O dicionario possui dois elementos, o tamanho que o hash possui e o proprio hash. O hash é utilizado para inserir e pesquisar palavras em uma estrutura com complexidade aproximadamente $O(1)$ para inserções e para pesquisas de termos. Cada indice do hash é o pronteiro para a celula cabeça de uma fila de palavras
\begin{algorithm}[h!]
\begin{footnotesize}

	Dicionario
	{
		Hash 
  		tamanho
	}
	
\end{footnotesize}
\end{algorithm}

\subsubsection{Fila de palavras:} 

A fila de palavras guarda todas as palavras nas quais a funcao H do hash calculou para aquela posição. Por ser uma fila a complexidade de pesquisa da mesma é $O(n)$, mas para um hash balanceado espera-se que esta fila possua poucos elementos, ou seja tenha acesso praticamente constante. Cada celula da fila de palavras possui um item que por sua vez contém a palavra inserida e uma fila de documentos que possui aquela palavra.
\begin{algorithm}[h!]
\begin{footnotesize}

	Fila de palavras
	{
		Item
		{
			Fila de documentos 
	  		palavra
		}
	}
	
\end{footnotesize}
\end{algorithm}

\subsubsection{Fila de documentos:} 

A fila de documentos guarda todos os ids dos documentos inseridos e o número de vezes que este documento foi inserido para cada palavra, este parametro é utilizado para descobrir quais elementos estão presentes em duas pesquisas.
\begin{algorithm}[h!]
\begin{footnotesize}

	Fila de documentos
	{
		Item
		{
			int idDoc;
    			int numInsercoes;
		}
	}
	
\end{footnotesize}
\end{algorithm}


















\subsection{Algoritmos}
\subsubsection{Simulação}

Controla a execussão do programa principal. Realiza um loop $S$ vezes, onde $S$ é um parâmetro enviado na entrada. Como realiza as chamadas das funções que executam o programa, esta função é $O(S)$ x ($O(M + P)$ + $O(N*P*D)$). Obviamente o gargalo esta função é $O(N*P*D)$, logo podemos considerar a função como sendo $O(S*N*P*D)$ sendo $N$ o número de pontos do arquivo de entrada, $P$ o número de pontos da amostra e $D$ o número de dimensões de um arquivo

\section{IMPLEMENTAÇÃO}
\label{implementacao}

\subsection{Código}

\subsubsection{Arquivos .c}

\begin{itemize}
\item \textbf{main.c:} Arquivo principal do programa que implementa o simulador de pontos.
\item \textbf{file.c:} Define as funções relacionadas à leitura e escrita de arquivos.
\item \textbf{fila.c:} Define as operações relacionadas à fila e à análise estatistica dos dados.
\end{itemize}

\subsubsection{Arquivos .h}

\begin{itemize}
\item \textbf{file.h:} Define os cabeçalhos das funções relacionadas à leitura e escrita de arquivos.
\item \textbf{fila.h:} Define as estruturas de dados e cabeçalhos de funções relacionadas a manipulação de um TAD fila implementada

\end{itemize}

\subsection{Compilação}

O programa deve ser compilado através do compilador GCC através de um makefile, ou pelo script na pasta scripts ou ainda do seguinte comando:

\begin{footnotesize}
\begin{verbatim} gcc main.c file.c fila.c -o aplicativo \end{verbatim}
\end{footnotesize}

\subsection{Execução}

A execução do programa tem como parâmetros:
\begin{itemize}
\item Um arquivo de entrada.
\item O tamanho de cada amostra.
\item O número de testes a serem realizados.
\item Um arquivo de saída.
\end{itemize}

O comando para a execução do programa é da forma:

\begin{footnotesize}
\begin{verbatim} ./aplicativo -i<arquivo de entrada> -s <número de testes> -n <tamanho da amostra> -o <arquivo de saida> \end{verbatim}
\end{footnotesize}

\subsubsection{Formato da entrada}

O formato da entrada possui dois numeros na primeira linha, a saber: numero de pontos, numero de dimensoes
E existirao outras (numero de pontos) linhas no arquivo, uma para cada ponto. Os valores de cada ponto devem ficar entre 0 e 1. A leitura dos argumentos foi feita atravéz da função getOpt  \cite{GetOpt} 
\begin{footnotesize}
\begin{verbatim}
4 2
0.00 0.21
1.00 0.45
0.54 0.89
0.17 0.64
\end{verbatim}
\end{footnotesize}

\subsubsection{Formato da saída}

A saída do programa exibe as estatisticas feitas durante a execusaao, a saber:
linha 1: valor minimo
linha 2: valor maximo
linha 3: valor medio
linha 4: desvio padrao 
exemplo de saida:

\begin{footnotesize}
\begin{verbatim}
0.638654
0.644165
0.641232
0.010553
\end{verbatim}

Uma abordagem mais detalhada sobre desvio padrão pode ser encontrada em: \cite{DesvioPadrao}
\end{footnotesize}


\section{AVALIAÇÃO EXPERIMENTAL}
\label{avaliacao_experimental}

Nesta parte foram feitas duas avaliações:
A primeira é a comparação entre tempo de execussao e tempo decorrido no relógio, a segunda comparação é entre a variação dos valores no cálculo da estatística.

\subsubsection{Variação do tempo}

Em todos os testes execultados ocorreu uma variação entre o tempo de execussão e o tempo de relógio, salvo é claro quando a entrada dos arquivos era muito pequena, pois neste caso o instrumento de medida não era capaz de avaliar o tempo decorrido. Mas, para as entradas na pasta 'entradas' este tempo é medido. Exemplo: (execussao do programa com entrada: normal.data S = 100 N = 100) O arquivo timeAnalisis possui os tempos de execussão de todos os testes rodados. Note que neste caso há uma discrepância entre o que foi dito aqui em cima e o resultado no arquivo. Em alguns casos, o tempo de relógio registrado foi inferior ao tempo execultado pela máquina. Na realidade o que aconteceu é que a precisão da função gerrusages é double e da função getdateoftime é long. Nos casos em que o tempo de relogio é menor do que o tempo de máquina ocorreram somente por erro de precisão do long em relaçao ao double. Pois como já explicado este tempo não pode ser menor do que o tempo de maquina.

Algumas entradas encontradas no arquivo:
\begin{table}[ht!]
\centering
\begin{footnotesize}
\begin{tabular}{|c|c|}
\hline
\textbf{\#Tempo execussao(segundos)} & \textbf{Tempo Relogio(segundos)} \\ \hline
515.696228 & 517.000000\\
515.700229 & 518.000000\\
774.004371 & 776.000000\\
772.284264 & 772.000000\\
771.692227 & 773.000000\\
\end{tabular}
\end{footnotesize}
\caption{Tempo de execussão e tempo de relógio para uma das execussões do programa \label{tempo execussao}}
\end{table}

\subsubsection{Análise estatística}

A análise estatística consiste em analisar para um dado conjunto de dados aqueles que provavelmente se agrupam ou não com os outros pontos gerados. Nesta parte do programa foram realizados vários testes com três amostras diferentes de arquivo, três valores de S e N diferentes. Todas as combinações possíveis entre S e N foram testadas, para que a avaliação fosse justa com as várias baterias de testes, o parametro para a função SRAND foi o mesmo em todos eles = 1234. Todos os testes foram feitos em um computador pessoal. Sistema operacional Ubuntu 9.10, processador intel core 2 duo 2gb de memória RAM.

Comparando os arquivos timeAnalisis.txt e as entradas S e N presentes no arquivo scripts/make percebemos que quanto maior o valor de N maior o tempo de execussão, assim como S. Isto é perfeitamente aceitável, pois, Aumentando-se o número de iterações do loop, mais amostras são geradas e mais tempo processando será necessário. Assim como quanto maior o o tamanho de N (tamanho das amostras) mais pontos serão analizados entre si no gargalo do algoritmo que é $O(N2)$.

Pela análise dos testes gerados podemos concluir que em geral, quanto maio o valor das amostras retiradas e quanto maior o número de iterações mais próximo do resultado esperado a amostra fica. Para este teste em específico, era esperado que o arquivo clustering.data fosse o menos agrupável de todos, pois os pontos contidos neste arquivo possuem uma certa concentração mas em algumas partes, enquanto em outras ele é totalmente esparço. Para o arquivo normal.data o resultado esperado é que ele estivesse na média dos três arquivos pois os valores contidos neste arquivo não estão concentrados, mas também não estão uniformemente distribuidos. Por fim, o arquivo uniform.data era o que esperava-se ser o mais agrupável de todos, uma vez que tanto ele quanto o gerador de números pseudo-aleatórios da linguagem C são constituidos de pontos uniformemente distribuidos. De fato, podemos observar nos arquivos presentes na pasta resultados que o grau de agrupamento dos pontos ocorre em ordem crescente do clustering para o normal e do normal para o uniform.

\section{CONCLUSÃO}
\label{conclusao}

Foi descrito neste trabalho um simulador de agrupamento de pontos utilizando a tecnica de monte carlo. A tecnica empregada possui uma eficiencia estatistica relativamente grande, uma vez que não necessitamos analizar toda a entrada de dados para descobrir se um conjunto de pontos é possivelmente agrupavel, o que neste caso poupa muito tempo em outras análises de detalhes mais práticos em outras aplicações.

O trabalho ainda discutiu a diferença entre tempo de máquina e tempo de relógio, foi mostrado experimentalmente aquilo que era esperado: o tempo que o programa gastou para ser execultado é menor do que o tempo decorrido no relógio.

O trabalho atingiu seus principais objetivos: a prática da linguagem de programação C e o estudo da análise de monte carlo. A parte que mais demandou tempo foi entender no que consistia tal análise, e como implementá-la, pois a implementação em si é fácil para quem já domina a linguagem.

A análise de complexidade da solução foi relativamente simples, uma vez que a estrutura de dados utilizada não foi a mais complexa possivel para utilizar no problema, o que neste caso facilitou.

Algumas melhorias que poderiam ser consideradas neste trabalho são:
\begin{itemize}
\item O emprego de uma outra estrutura de dados do tipo arvore, que permite que os calculos das distancias entre os pontos seja calculada de maneira mais rápida.
\item Ao gerar um ponto randomicamente já calcular sua distância com os demais pontos do arquivo, assim seria possível economizar memória
\end{itemize}
\bibliographystyle{sbc}
\bibliography{tp0}

\end{document}
